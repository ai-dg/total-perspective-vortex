
https://physionet.org/content/eegmmidb/1.0.0/

| Run     | Task                  | RÃ©el / Imaginaire | mouvement                 | T1 = ?      | T2 = ?      |
| ------- | --------------------- | ----------------- | ------------------------- | ----------- | ----------- |
| **R01** | Baseline yeux ouverts | â€”                 | aucun                     | â€”           | â€”           |
| **R02** | Baseline yeux fermÃ©s  | â€”                 | aucun                     | â€”           | â€”           |
| **R03** | **Task 1**            | **rÃ©el**          | main gauche / main droite | main gauche | main droite |
| **R04** | **Task 2**            | â­ **imaginaire**  | main gauche / main droite | main gauche | main droite |
| **R05** | **Task 3**            | rÃ©el              | mains / pieds             | deux mains  | deux pieds  |
| **R06** | **Task 4**            | â­ **imaginaire**  | mains / pieds             | deux mains  | deux pieds  |
| **R07** | Task 1                | rÃ©el              | main gauche / main droite | main gauche | main droite |
| **R08** | Task 2                | â­ imaginaire      | main gauche / main droite | main gauche | main droite |
| **R09** | Task 3                | rÃ©el              | mains / pieds             | deux mains  | deux pieds  |
| **R10** | Task 4                | â­ imaginaire      | mains / pieds             | deux mains  | deux pieds  |
| **R11** | Task 1                | rÃ©el              | main gauche / main droite | main gauche | main droite |
| **R12** | Task 2                | â­ imaginaire      | main gauche / main droite | main gauche | main droite |
| **R13** | Task 3                | rÃ©el              | mains / pieds             | deux mains  | deux pieds  |
| **R14** | Task 4                | â­ imaginaire      | mains / pieds             | deux mains  | deux pieds  |


| Experiment ID | Run |
| ------------- | --- |
| 0             | R03 |
| 1             | R04 |
| 2             | R05 |
| 3             | R06 |
| 4             | R07 |
| 5             | R08 |


| Bande     | FrÃ©quence  | Signification               |
| --------- | ---------- | --------------------------- |
| **Delta** | 0.5 â€“ 4 Hz | sommeil profond             |
| **Theta** | 4 â€“ 8 Hz   | somnolence, mÃ©ditation      |
| **Alpha** | 8 â€“ 12 Hz  | repos, yeux fermÃ©s          |
| **Beta**  | 12 â€“ 30 Hz | activitÃ© mentale, mouvement |
| **Gamma** | 30â€“100 Hz  | cognition complexe          |


Avec :

64 = 64 channels EEG

N = nombre total dâ€™Ã©chantillons dans le run

Exemple rÃ©el :

160 Hz = 160 Ã©chantillons par seconde

un run dure ~2 minutes â†’ 120 s
â†’ donc N â‰ˆ 160 Ã— 120 = 19 200 colonnes

[ 64 channels ]  x  [ ~19000 Ã©chantillons (dans le temps) ]



1 / 160 = 0.00625 s par Ã©chantillon


High-pass â†’ laisse passer les hautes frÃ©quences

Low-pass â†’ laisse passer les basses frÃ©quences

âœ” highpass: 0.0 Hz

Ã‡a veut dire quâ€™ils nâ€™ont pas appliquÃ© de filtre coupe-bas matÃ©riel.
Le dataset garde mÃªme les trÃ¨s basses frÃ©quences (0â€“1 Hz), qui contiennent :

mouvement des yeux

respiration

dÃ©rive du signal

fluctuations lentes

âœ” lowpass: 80.0 Hz

Ã‡a veut dire que le matÃ©riel a coupÃ© toutes les frÃ©quences au-dessus de 80 Hz.

Car au-dessus de 80â€“100 Hz, lâ€™EEG scalp est presque uniquement du bruit musculaire, pas de lâ€™activitÃ© neuronale.

Donc la bande 0â€“80 Hz est ce qui reste dans les fichiers .edf.

ğŸ‘‰ Câ€™est le filtrage matÃ©riel, pas ton filtrage logiciel.


âœ” 0â€“4 Hz = delta (sommeil lent)
âœ” 4â€“8 Hz = theta
âœ” 8â€“12 Hz = mu / alpha (super utile pour motor imagery !)
âœ” 12â€“30 Hz = beta (encore plus utile !)
âœ” 30â€“80 Hz = gamma (souvent bruit musculaire)


Tes donnÃ©es actuelles (non filtrÃ©es) contiennent :

dÃ©rive lente (0â€“1 Hz)

yeux clignÃ©s (1â€“5 Hz)

mouvements de tÃªte

ondes alpha (8â€“12 Hz)

ondes beta (12â€“30 Hz)

bruit musculaire (30â€“60 Hz)

un peu de gamma (jusquâ€™Ã  80 Hz)

ParamÃ¨tre	Signification
highpass: 0 Hz	pas de filtre coupe-bas matÃ©riel (conserve les trÃ¨s basses frÃ©quences)
lowpass: 80 Hz	le matÃ©riel EEG coupe tout au-dessus de 80 Hz
raw.filter(8,30)	ton filtre logiciel â†’ garde les frÃ©quences motrices
raw.notch_filter(50)	retire le bruit Ã©lectrique (optionnel)

1 Hz = 1 oscillation par seconde
ğŸ”¹ La hauteur de ses sauts = amplitude = ÂµV
ğŸ”¹ Le nombre de sauts par seconde = frÃ©quence = Hz

| **Bande**          | **FrÃ©quence (Hz)** | **Nom / Fonction**                               | **Lien avec le mouvement (rÃ©el ou imaginaire)**                                          |
| ------------------ | ------------------ | ------------------------------------------------ | ---------------------------------------------------------------------------------------- |
| **Delta**          | 0.5 â€“ 4 Hz         | sommeil profond                                  | pas utile (bruit, dÃ©rive lente)                                                          |
| **Theta**          | 4 â€“ 8 Hz           | relaxation, navigation, mÃ©moire                  | faible lien (un peu dâ€™imagerie motrice)                                                  |
| **Alpha (Âµ / Mu)** | **8 â€“ 12 Hz**      | **rythme sensorimoteur (SMR)**                   | â­ **diminue fortement (ERD) quand tu imagines ou fais un mouvement**, surtout dans C3/C4 |
| **Beta**           | **12 â€“ 30 Hz**     | activitÃ© motrice, contrÃ´le fin, retour sensoriel | â­ **augmente (ERS) aprÃ¨s ou pendant lâ€™imagination/mouvement**, trÃ¨s utile pour CSP       |
| **Gamma**          | 30 â€“ 80 Hz         | cognition haute frÃ©quence                        | peu utile en scalp EEG, contaminÃ© par EMG (muscles)                                      |
| **Haut Gamma**     | >80 Hz             | potentiel local (LFP)                            | non significatif en EEG classique (trop bruitÃ©)                                          |


| Ã‰lectrode | Zone                 | Fonction principale                            | CÃ´tÃ©   |
| --------- | -------------------- | ---------------------------------------------- | ------ |
| **C3**    | Cortex moteur gauche | Imagination du **mouvement de la main droite** | Gauche |
| **C4**    | Cortex moteur droit  | Imagination du **mouvement de la main gauche** | Droit  |
| **Cz**    | Ligne mÃ©diane        | Point central, contrÃ´le tronc/jambes           | Centre |


| Ã‰lectrode | Zone               | RÃ´le                                   |
| --------- | ------------------ | -------------------------------------- |
| **FC3**   | PrÃ©-moteur gauche  | PrÃ©paration du mouvement (main droite) |
| **FC4**   | PrÃ©-moteur droit   | PrÃ©paration du mouvement (main gauche) |
| **CP3**   | Post-moteur gauche | Retour sensoriel (main droite)         |
| **CP4**   | Post-moteur droit  | Retour sensoriel (main gauche)         |


| Ã‰lectrode     | Zone               | RÃ´le                            |
| ------------- | ------------------ | ------------------------------- |
| **C1 / C2**   | proches C3/C4      | Variation latÃ©rale              |
| **FC1 / FC2** | prÃ©-moteur mÃ©dial  | Confirme la prÃ©paration motrice |
| **CP1 / CP2** | post-moteur mÃ©dial | IntÃ©gration sensorielle         |
| **C5 / C6**   | pÃ©riphÃ©rique       | Mouvement bras / Ã©paule         |


| CatÃ©gorie                | Ã‰lectrodes         | RÃ´le                                      |
| ------------------------ | ------------------ | ----------------------------------------- |
| **Critiques**            | C3, C4, Cz         | C3=main droite, C4=main gauche, Cz=centre |
| **Autour (importants)**  | FC3, FC4, CP3, CP4 | PrÃ©-moteur et sensorimoteur               |
| **PÃ©riphÃ©riques utiles** | C1, C2, C5, C6     | Contributions latÃ©rales                   |
| **Renfort**              | FC1, FC2, CP1, CP2 | Contribution mÃ©diane                      |

---
```bash
def manual_cov(X):
    Xc = X - X.mean(axis=0)
    return (Xc.T @ Xc) / (Xc.shape[0] - 1)



C1 = np.cov(X, rowvar=False)
C2 = manual_cov(X)
np.allclose(C1, C2, atol=1e-8)
```

```bash
vals_np, vecs_np = np.linalg.eigh(C)   # C symÃ©trique
vals_my, vecs_my = manual_eigendecomposition(C)
# eigenvalues
np.allclose(np.sort(vals_np), np.sort(vals_my), atol=1e-6)
for i in range(k):  # k premiÃ¨res valeurs propres
    v1 = vecs_np[:, i]
    v2 = vecs_my[:, i]
    cos = abs(np.dot(v1, v2))  # â‰ˆ 1 si mÃªmes directions
    assert cos > 0.99
---
U_np, S_np, Vt_np = np.linalg.svd(X, full_matrices=False)
U_my, S_my, Vt_my = manual_svd(X)
X_rec = U_my @ np.diag(S_my) @ Vt_my
np.allclose(X, X_rec, atol=1e-6)
np.allclose(np.sort(S_np), np.sort(S_my), atol=1e-6)

```
---
```bash
W_np = csp_numpy.filters_   # (n_filters, n_channels)
W_my = csp_manual.filters_

# Normaliser et comparer quelques colonnes via |cos(angle)|
for i in range(n_filters):
    v1 = W_np[i] / np.linalg.norm(W_np[i])
    v2 = W_my[i] / np.linalg.norm(W_my[i])
    cos = abs(np.dot(v1, v2))
    print(i, cos)  # â‰ˆ 1 si Ã©quivalent
np.allclose(features_numpy, features_manual, atol=1e-4)
pipe_np = Pipeline([("csp", csp_numpy), ("clf", LogisticRegression())])
pipe_my = Pipeline([("csp", csp_manual), ("clf", LogisticRegression())])

acc_np = cross_val_score(pipe_np, X_flat, y, cv=5).mean()
acc_my = cross_val_score(pipe_my, X_flat, y, cv=5).mean()

print(acc_np, acc_my)
```


---
## Results

temps (s)= sample / 160

672 samples / 160 Hz = 4.2 secondes


[ 672     0     3 ]
   â”‚      â”‚      â””â”€â”€ code Ã©vÃ©nement (3 = T2)
   â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€ col. inutilisÃ©e
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ sample index


[event_sample, previous_event_code, new_event_code]


| Code   | Signification | DÃ©tails                                                                                                                                                   | UtilisÃ© pour la classification main gauche/droite ? |
| ------ | ------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- |
| **T0** | **Repos**     | Le sujet ne bouge pas, il ne fait pas d'imagerie motrice                                                                                                  | âŒ Non (Ã  ignorer pour la classification MI)         |
| **T1** | **Classe 1**  | *Selon le type de run :* <br>â€¢ Runs 3, 7, 11 : **Mouvement/Imagination de la main gauche** <br>â€¢ Runs 5, 9, 13 : **Mouvement/Imagination des deux mains** | âœ”ï¸ Oui pour main gauche (runs 3,7,11)               |
| **T2** | **Classe 2**  | *Selon le type de run :* <br>â€¢ Runs 3, 7, 11 : **Mouvement/Imagination de la main droite** <br>â€¢ Runs 5, 9, 13 : **Mouvement/Imagination des deux pieds** | âœ”ï¸ Oui pour main droite (runs 3,7,11)               |

| Code   | InterprÃ©tation gÃ©nÃ©rale | InterprÃ©tation spÃ©cifique (runs 3,4,7,8,11,12) |
| ------ | ----------------------- | ---------------------------------------------- |
| **T0** | Repos (rien)            | Aucun mouvement â†’ ignorer                      |
| **T1** | ActivitÃ© classe 1       | **Main gauche (rÃ©elle ou imagÃ©e)**             |
| **T2** | ActivitÃ© classe 2       | **Main droite (rÃ©elle ou imagÃ©e)**             |


| Run    | Type                                | T1     | T2     | âœ”ï¸ Utilisable ? |
| ------ | ----------------------------------- | ------ | ------ | --------------- |
| **3**  | Mouvement rÃ©el main gauche/droite   | Gauche | Droite | âœ”ï¸              |
| **4**  | Imagination main gauche/droite      | Gauche | Droite | âœ”ï¸              |
| **5**  | Mouv. rÃ©el deux mains / deux pieds  | Mains  | Pieds  | âŒ               |
| **6**  | Imagination deux mains / deux pieds | Mains  | Pieds  | âŒ               |
| **7**  | Mouvement rÃ©el main gauche/droite   | Gauche | Droite | âœ”ï¸              |
| **8**  | Imagination main gauche/droite      | Gauche | Droite | âœ”ï¸              |
| **9**  | Mouv. rÃ©el deux mains / deux pieds  | Mains  | Pieds  | âŒ               |
| **10** | Imagination deux mains / deux pieds | Mains  | Pieds  | âŒ               |
| **11** | Mouvement rÃ©el main gauche/droite   | Gauche | Droite | âœ”ï¸              |
| **12** | Imagination main gauche/droite      | Gauche | Droite | âœ”ï¸              |
| **13** | Mains / pieds                       | Mains  | Pieds  | âŒ               |
| **14** | Mains / pieds                       | Mains  | Pieds  | âŒ               |


|--- T0 ---|====T2====|---T0---|====T2====|---T0---|====T2====|
 samples
 0        672        1328      4656      5312      5984    etc.


tmin = -0.5, tmax = 4.0


[0.5 sec avant T2] â†’ [4 sec aprÃ¨s T2]


https://mindbigdata.com/opendb/index.html

https://www.physionet.org/content/sleep-edfx/1.0.0/

---
# CSP Eigenvalues sorting

â€œNote that Î»(c)_j â‰¥ 0 is the variance in condition c in the corresponding surrogate channel and Î»(+)_j + Î»(-)_j = 1.â€

â€œHence a large value Î»(+)_j close to one indicates that the corresponding spatial filter w_j yields high variance in the positive condition and low variance in the negative condition.â€

(source: p.4)

Î»(+)_j grand â†’ variance Ã©levÃ©e pour la classe + â†’ variance faible pour la classe âˆ’

Î»(+)_j petit â†’ variance Ã©levÃ©e pour la classe âˆ’ â†’ variance faible pour la classe +


# CSP W matrix
ğŸ“Œ 1. Page 4 â€” Juste aprÃ¨s lâ€™Ã©quation (5)

Câ€™est ici que CSP est dÃ©fini mathÃ©matiquement.

Citation :

â€œLet W denote the matrix in which the rows give the filters.â€


â¡ï¸ Cela signifie :
chaque ligne de W = un filtre CSP = un eigenvector transposÃ©

ğŸ“Œ 2. Page 4 â€” La diagonalisation conjointe

Le PDF rappelle que :

ğ‘Š
ğ‘‡
Î£
(
+
)
ğ‘Š
=
Î›
(
+
)
W
T
Î£(+)W=Î›(+)
ğ‘Š
ğ‘‡
Î£
(
âˆ’
)
ğ‘Š
=
Î›
(
âˆ’
)
W
T
Î£(âˆ’)W=Î›(âˆ’)

Cela implique :

ğŸ‘‰ W est formÃ©e des eigenvectors du problÃ¨me gÃ©nÃ©ralisÃ©

Î£
+
ğ‘¤
=
ğœ†
Î£
âˆ’
ğ‘¤
Î£
+
w=Î»Î£
âˆ’
w

Câ€™est exactement ce que tu as dÃ©jÃ  rÃ©solu.

ğŸ“Œ 3. Page 5 â€” SÃ©lection des filtres extrÃªmes

Citation :

â€œThe first and last few eigenvectors yield filters with maximal discriminative power.â€


Ceci est le point essentiel pour construire W :

Prendre k eigenvectors associÃ©s aux plus petites eigenvalues

Prendre k eigenvectors associÃ©s aux plus grandes eigenvalues

Puis les empiler dans W.


## Obtains features csp signals

ğŸ“˜ 1. Page 3 â€” La transformation CSP & les features log-variance

ğŸ‘‰ Câ€™est ici que la partie â€œW @ X puis log-varianceâ€ est dÃ©crite.

Sur la page 3, section â€œA. Optimization Principlesâ€ et surtout Fig. 2, tu trouves :

â€œVariance of the projected sources is used as features.â€
â€œLogarithmic power (log-variance) is commonly used as feature vector.â€


ğŸ“Œ Câ€™est exactement ton code :

z_i = W @ X_i
f_i = np.log(np.var(z_i, axis=1))

ğŸ“˜ 2. Page 4 â€” Construction du filtre W et projection du signal

Dans la phrase juste aprÃ¨s lâ€™Ã©quation (5), le PDF dit :

â€œW denotes the matrix whose rows give the filters.â€


Cela veut dire :

chaque ligne de W = un filtre CSP

ce filtre est appliquÃ© directement au signal EEG :

ğ‘
=
ğ‘Š
ğ‘‹
Z=WX

Câ€™est mot pour mot la projection que tu fais :

z_i = W @ X_i

ğŸ“˜ 3. Page 5 â€” SÃ©lection des filtres extrÃªmes (donc composition de W)

â€œThe first and last few eigenvectors yield filters with maximal discriminative power.â€


Cela justifie pourquoi W contient :

les k eigenvectors avec petits eigenvalues

les k eigenvectors avec grands eigenvalues

Ce qui donne W dans ton code prÃ©cÃ©dent.


1ï¸âƒ£ Dâ€™oÃ¹ viennent alors les â€œ6 experimentsâ€ ?

Dans le PDF 42, â€œexperiment 0â€¦5â€ ne veut pas dire â€œil existe 6 types de tÃ¢ches dans le datasetâ€, mais :

6 configurations de classification diffÃ©rentes construites Ã  partir des 4 tÃ¢ches moteurs.

Une faÃ§on trÃ¨s naturelle (et cohÃ©rente avec le protocole) de dÃ©finir ces 6 expÃ©riences est par ex. :

Exp 0 : Task 1 â€“ rÃ©el main gauche vs main droite
â†’ runs 3, 7, 11 (T1=LG, T2=RD)

Exp 1 : Task 2 â€“ imaginer main gauche vs main droite
â†’ runs 4, 8, 12

Exp 2 : Task 1+2 combinÃ©s (rÃ©el+imag) gauche vs droite
â†’ runs 3,4,7,8,11,12 (T1=LG, T2=RD)

Exp 3 : Task 3 â€“ rÃ©el poings vs pieds
â†’ runs 5, 9, 13 (T1=poings, T2=pieds)

Exp 4 : Task 4 â€“ imaginer poings vs pieds
â†’ runs 6, 10, 14

Exp 5 : Task 3+4 combinÃ©s (rÃ©el+imag) poings vs pieds
â†’ runs 5,6,9,10,13,14 (T1=poings, T2=pieds)

ğŸ‘‰ Dans tous les cas, chaque experiment reste un problÃ¨me binaire T1 vs T2, simplement avec un choix diffÃ©rent de runs / conditions (rÃ©el / imagÃ© / mix).

Tu nâ€™utilises jamais T0 dans ces expÃ©riences.

Ã€ partir de la description que tu as collÃ©e :

runs 3,4,7,8,11,12 :

T1 = left fist, T2 = right fist

runs 5,6,9,10,13,14 :

T1 = both fists, T2 = both feet

Je te proposais de construire les 6 experiments ainsi :

Exp 0 : Task 1 rÃ©el (main G/D) â†’ runs 3,7,11

Exp 1 : Task 2 imagÃ© (main G/D) â†’ runs 4,8,12

Exp 2 : Task 1+2 mix (G/D rÃ©el+imagÃ©) â†’ runs 3,4,7,8,11,12

Exp 3 : Task 3 rÃ©el (poings/pieds) â†’ runs 5,9,13

Exp 4 : Task 4 imagÃ© (poings/pieds) â†’ runs 6,10,14

Exp 5 : Task 3+4 mix (poings/pieds rÃ©el+imagÃ©) â†’ runs 5,6,9,10,13,14



Exp 0 â†’ Train = 3 ; Test = 7,11
Exp 1 â†’ Train = 4 ; Test = 8,12
Exp 2 â†’ Train = 3 ; Test = 4,7,8,11,12
Exp 3 â†’ Train = 5 ; Test = 9,13
Exp 4 â†’ Train = 6 ; Test = 10,14
Exp 5 â†’ Train = 5 ; Test = 6,9,10,13,14


Notes CSP â€” Soutenance MyCSP (42)

## 1. PrÃ©traitement
- Band-pass (ex. 8â€“30 Hz) pour isoler les rythmes moteurs (Âµ / Î²) â†’ ERD/ERS.
- DÃ©coupage en epochs autour des Ã©vÃ©nements (ex. âˆ’0.5s â†’ 4s).
â†’ **But : augmenter le rapport signal/bruit et capturer lâ€™activitÃ© liÃ©e Ã  T1/T2.**

## 2. Covariances
- Pour chaque classe : calcul dâ€™une matrice de covariance normalisÃ©e.
- Normalisation par la trace :
Î£ â† Î£ / trace(Î£)
â†’ **Covariance = information spatiale du cerveau + patterns propres Ã  chaque tÃ¢che.**

## 3. ProblÃ¨me aux valeurs propres (CSP)
- RÃ©solution : Î£â‚Š w = Î» Î£â‚‹ w
- Î» = ratio de variance entre classes aprÃ¨s projection.
- Grand Î» â†’ variance forte pour classe +
- Petit Î» â†’ variance forte pour classe âˆ’
â†’ **Les eigenvectors = directions spatiales optimales.**

## 4. SÃ©lection des filtres CSP
- On trie les eigenvalues.
- On prend k plus petits eigenvectors + k plus grands eigenvectors.
â†’ Les valeurs du milieu ne sont pas discriminantes.
â†’ Matrice : W âˆˆ â„^{2k Ã— n_channels}

## 5. Projection CSP
- Pour chaque epoch : záµ¢ = W Xáµ¢
- Dimensions : (2k filtres Ã— n_samples)
â†’ Chaque ligne = un filtre spatial qui maximise/minimise la variance selon la classe.

## 6. Extraction des features
- fáµ¢[j] = log(Var(záµ¢[j]))
â†’ **Variance = information discriminante.**
â†’ **Log = stabilisation, compression, meilleure linÃ©aritÃ©.**

## 7. Classification
- Features â†’ LogisticRegression (ou LDA).
â†’ **CSP + log(var) = sÃ©paration linÃ©aire entre classes.**

## RÃ©sumÃ© court
Je filtre le signal, jâ€™extrais les epochs, je calcule les covariances par classe,
puis je rÃ©sous un problÃ¨me gÃ©nÃ©ralisÃ© dâ€™eigenvalues pour obtenir les directions
oÃ¹ la variance est maximale pour une classe et minimale pour lâ€™autre.
Je sÃ©lectionne les eigenvectors extrÃªmes, je projette les epochs, jâ€™extrais le
log-variance et jâ€™entraÃ®ne un classifieur dessus.




# Common Spatial Patterns (CSP) â€“ Eigenvalues & Eigenvectors  
## Formules, explications et rÃ©solution mathÃ©matique

Ce document dÃ©crit clairement les formules utilisÃ©es dans CSP pour la
dÃ©composition en valeurs propres (eigenvalues) et vecteurs propres (eigenvectors),
dans le cadre du problÃ¨me gÃ©nÃ©ralisÃ© entre les matrices de covariance Î£âº et Î£â».

---

# 1. Matrices Î£âº et Î£â» (Covariances moyennes)

Ã€ partir des epochs EEG filtrÃ©s :

- Xi : epoch i (shape : channels Ã— time)
- Yi : label (classe + ou -)

Pour chaque epoch, on calcule sa covariance spatiale :

```
Ci = Xi * Xiáµ€
```

Puis on normalise par la trace :

```
Ci_norm = Ci / trace(Ci)
```

On sÃ©pare les epochs par classe :

```
Î£âº = moyenne des Ci_norm pour la classe +
Î£â» = moyenne des Ci_norm pour la classe -
```

Ces deux matrices rÃ©sument la structure spatiale de chaque classe.

---

# 2. ProblÃ¨me gÃ©nÃ©ralisÃ© dâ€™autovaleurs (Generalized Eigenvalue Problem)

Dans CSP, on rÃ©sout :

```
Î£âº w = Î» Î£â» w
```

Ce problÃ¨me trouve des vecteurs w qui :

- maximisent la variance pour Î£âº tout en la minimisant pour Î£â»
- ou lâ€™inverse (pour les petits Î»)

Ce sont les directions spatiales discriminantes.

---

# 3. Passage Ã  une forme standard

Le problÃ¨me gÃ©nÃ©ralisÃ© Ã©quivaut Ã  :

```
Î£â»â»Â¹ Î£âº w = Î» w
```

Ce qui est une dÃ©composition propre classique.

---

# 4. DÃ©composition en valeurs propres

La dÃ©composition dâ€™une matrice A en valeurs propres consiste Ã  trouver :

```
A v = Î» v
```

Les vecteurs v sont invariants par la transformation A (changÃ©s seulement en Ã©chelle).

En regroupant tous les eigenvectors :

```
A V = V Î›
```

avec :

- V : matrice des eigenvectors (colonnes)
- Î› : matrice diagonale des eigenvalues

Si V est inversible :

```
A = V Î› Vâ»Â¹
```

---

# 5. Application au CSP

On applique la dÃ©composition gÃ©nÃ©ralisÃ©e :

```
eigvals, eigvecs = eig(Î£âº, Î£â»)
```

oÃ¹ :

- eigvals : Î»â‚ â€¦ Î»â‚™ (shape : n,)
- eigvecs : matrice V contenant les eigenvectors (shape : n Ã— n)

---

# 6. Tri des eigenvectors

Les eigenvalues sont triÃ©s par ordre dÃ©croissant :

```
Î»â‚ â‰¥ Î»â‚‚ â‰¥ ... â‰¥ Î»â‚™
```

et on rÃ©organise les eigenvectors en consÃ©quence.

Les eigenvectors associÃ©s aux :

- plus grands Î» â†’ maximisent la variance pour la classe +
- plus petits Î» â†’ maximisent la variance pour la classe â€“

---

# 7. Matrice de projection CSP (W)

En pratique, on sÃ©lectionne les k vecteurs propres aux extrÃ©mitÃ©s :

```
W = [vâ‚ ... v_k, v_(nâˆ’k+1) ... v_n]áµ€
```

W est la matrice finale CSP utilisÃ©e pour projeter les signaux :

```
Z = W X
```

Z contient les canaux virtuels discriminants.

---

# 8. RÃ©sumÃ© rapide (pour soutenance)

- Î£âº et Î£â» = covariances moyennes par classe  
- CSP rÃ©sout : Î£âº w = Î» Î£â» w  
- Î» = ratio de variance entre classes  
- w = direction spatiale maximisant ou minimisant la variance  
- On trie les eigenvalues et on garde les plus extrÃªmes  
- W = matrice CSP  
- Z = features CSP



# Common Spatial Patterns (CSP) â€“ ProblÃ¨me gÃ©nÃ©ralisÃ© dâ€™autovaleurs  
## Formules avec notation $$ ... $$ (LaTeX compatible Markdown)

Ce document prÃ©sente les formules utilisÃ©es dans CSP pour rÃ©soudre le problÃ¨me gÃ©nÃ©ralisÃ© dâ€™autovaleurs entre les matrices de covariance de deux classes, en utilisant la notation LaTeX avec `$$`.

---

# 1. Covariance spatiale normalisÃ©e

Pour chaque epoch \(X_i \in \mathbb{R}^{C \times T}\) :

$$
C_i = X_i X_i^T
$$

Normalisation :

$$
\widetilde{C}_i = \frac{C_i}{\operatorname{trace}(C_i)}
$$

---

# 2. Covariances moyennes des deux classes

Soit :

- \( \mathcal{I}_+ \) = indices des epochs de la classe +
- \( \mathcal{I}_- \) = indices des epochs de la classe âˆ’

Alors :

$$
\Sigma^{(+)} = \frac{1}{|\mathcal{I}_+|} \sum_{i \in \mathcal{I}_+} \widetilde{C}_i
$$

$$
\Sigma^{(-)} = \frac{1}{|\mathcal{I}_-|} \sum_{i \in \mathcal{I}_-} \widetilde{C}_i
$$

---

# 3. ProblÃ¨me gÃ©nÃ©ralisÃ© dâ€™autovaleurs du CSP

Le CSP rÃ©sout :

$$
\Sigma^{(+)} w = \lambda \, \Sigma^{(-)} w
$$

---

# 4. Transformation en problÃ¨me classique

Si \( \Sigma^{(-)} \) est inversible :

$$
\Sigma^{(-)-1} \Sigma^{(+)} w = \lambda w
$$

On dÃ©finit alors la matrice :

$$
M = \Sigma^{(-)-1} \Sigma^{(+)}
$$

et on rÃ©sout :

$$
M w = \lambda w
$$

---

# 5. DÃ©composition propre (Eigen decomposition)

On peut Ã©crire :

$$
M = V \Lambda V^{-1}
$$

oÃ¹ :

- \( V = [w_1 \ w_2 \ \cdots \ w_C] \) est la matrice des eigenvectors
- \( \Lambda = \mathrm{diag}(\lambda_1, \ldots, \lambda_C) \) est la matrice diagonale des eigenvalues

Chaque colonne de \(V\) est un vecteur propre \(w_i\), associÃ© Ã  un eigenvalue \(\lambda_i\).

---

# 6. Calcul via SciPy

En pratique, on utilise :

```python
eigvals, eigvecs = scipy.linalg.eig(S_plus, S_minus)
```

Ce qui rÃ©sout numÃ©riquement :

$$
\Sigma^{(+)} w = \lambda \, \Sigma^{(-)} w
$$

---

# 7. SÃ©lection des filtres spatiaux CSP

Les eigenvalues sont triÃ©es par ordre dÃ©croissant :

$$
\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_C
$$

- les plus grands \(\lambda\) correspondent Ã  des filtres oÃ¹ la variance est maximale pour la classe +
- les plus petits \(\lambda\) correspondent Ã  des filtres oÃ¹ la variance est maximale pour la classe âˆ’

On construit la matrice CSP en sÃ©lectionnant les eigenvectors extrÃªmes :

$$
W =
\begin{bmatrix}
w_1^T \\
w_2^T \\
\vdots \\
w_k^T \\
w_{C-k+1}^T \\
\vdots \\
w_C^T
\end{bmatrix}
$$

---

# 8. Projection finale des donnÃ©es

Pour un epoch \( X \in \mathbb{R}^{C \times T} \), la projection CSP est :

$$
Z = W X
$$

Les lignes de \(Z\) sont les composantes CSP (canaux virtuels discriminants) qui serviront de features pour la classification.

---

# 9. RÃ©sumÃ© compact

$$
\begin{aligned}
&\widetilde{C}_i = \frac{X_i X_i^T}{\operatorname{trace}(X_i X_i^T)} \\\\
&\Sigma^{(+)} = \frac{1}{N_+} \sum \widetilde{C}_i,
\quad
\Sigma^{(-)} = \frac{1}{N_-} \sum \widetilde{C}_i \\\\
&\Sigma^{(+)} w = \lambda \Sigma^{(-)} w \\\\
&M = \Sigma^{(-)-1} \Sigma^{(+)} \\\\
&M = V \Lambda V^{-1} \\\\
&W = \text{matrice formÃ©e des eigenvectors extrÃªmes} \\\\
&Z = W X
\end{aligned}
$$



# CSP â€“ RÃ©solution mathÃ©matique de `scipy.linalg.eig(A, B)`
## Comment SciPy calcule les eigenvalues et eigenvectors du problÃ¨me gÃ©nÃ©ralisÃ©

Ce document dÃ©crit **exactement** comment SciPy rÃ©sout le problÃ¨me gÃ©nÃ©ralisÃ© dâ€™autovaleurs :

$$
A w = \lambda B w
$$

oÃ¹ \(A\) et \(B\) sont les matrices de covariance moyennes des deux classes (64Ã—64 dans CSP).

---

# 1. ProblÃ¨me gÃ©nÃ©ralisÃ© dâ€™autovaleurs

Le CSP demande de rÃ©soudre :

$$
A w = \lambda B w
$$

oÃ¹ :

- \(A = \Sigma^{(+)}\) = covariance moyenne de la classe +  
- \(B = \Sigma^{(-)}\) = covariance moyenne de la classe âˆ’  
- \(w\) = vecteur propre (eigenvector)  
- \(\lambda\) = valeur propre (eigenvalue), ratio de variance entre classes

---

# 2. DÃ©composition de Cholesky de \(B\)

Comme \(B\) est symÃ©trique dÃ©finie positive, SciPy calcule :

$$
B = L L^T
$$

oÃ¹ \(L\) est triangulaire infÃ©rieure.

Ce facteur servira Ã  transformer le problÃ¨me gÃ©nÃ©ralisÃ© en un problÃ¨me standard.

---

# 3. Transformation (Â« whitening Â») pour Ã©liminer \(B\)

On pose :

$$
M = L^{-1} A L^{-T}
$$

et le changement de variable :

$$
w = L^{-T} u
$$

Substitution dans lâ€™Ã©quation gÃ©nÃ©rale :

\[
A w = \lambda B w
\]

donne aprÃ¨s simplification :

$$
M u = \lambda u
$$

On a converti le problÃ¨me gÃ©nÃ©ralisÃ© en un **problÃ¨me classique dâ€™autovaleurs**.

---

# 4. DÃ©composition propre standard

SciPy diagonalise ensuite la matrice :

$$
M = V \Lambda V^{-1}
$$

Ce qui signifie que chaque colonne \(u_i\) de \(V\) vÃ©rifie :

$$
M u_i = \lambda_i u_i
$$

Les \(\lambda_i\) sont les eigenvalues du CSP.

---

# 5. Retour aux eigenvectors du problÃ¨me gÃ©nÃ©ralisÃ©

SciPy reconstruit ensuite les eigenvectors originaux via :

$$
w_i = L^{-T} u_i
$$

Ainsi :

$$
A w_i = \lambda_i B w_i
$$

La matrice finale des eigenvectors fournie par SciPy est :

$$
W = L^{-T} V
$$

Chaque colonne de \(W\) est un filtre spatial CSP.

---

# 6. RÃ©sultat de `scipy.linalg.eig(A, B)`

L'appel :

```python
eigvals, eigvecs = scipy.linalg.eig(A, B)
```

retourne directement :

- `eigvals` = \( (\lambda_1, \ldots, \lambda_C) \)
- `eigvecs` =  
  $$ W = [w_1 \ w_2 \ \cdots \ w_C] $$

tels que :

$$
A w_i = \lambda_i B w_i
$$

SciPy effectue **automatiquement** toutes les Ã©tapes :

1. Cholesky de \(B\)  
2. Construction de \(M = L^{-1} A L^{-T}\)  
3. RÃ©solution du problÃ¨me standard \(Mu = \lambda u\)  
4. Reconstruction \(w = L^{-T} u\)  

---

# 7. RÃ©sumÃ© compact (pour soutenance)

$$
\begin{aligned}
& A w = \lambda B w \\
& B = L L^T \\
& M = L^{-1} A L^{-T} \\
& M u = \lambda u \\
& w = L^{-T} u \\
& W = [w_1 \ w_2 \ \cdots \ w_C]
\end{aligned}
$$

---

# Fichier prÃªt pour utilisation dans votre projet CSP.